{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 132752,
     "status": "ok",
     "timestamp": 1743940553146,
     "user": {
      "displayName": "Anjana B Reghu",
      "userId": "05439852886024039531"
     },
     "user_tz": -330
    },
    "id": "vqXFRUMXZKWv"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
    "!pip install flask-cors\n",
    "!pip install pyngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37049,
     "status": "ok",
     "timestamp": 1743940590205,
     "user": {
      "displayName": "Anjana B Reghu",
      "userId": "05439852886024039531"
     },
     "user_tz": -330
    },
    "id": "7cxHLVpZZSNA",
    "outputId": "4fcadd07-36ed-4da5-f7be-a6a1c2c26141"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1397,
     "status": "ok",
     "timestamp": 1743940591600,
     "user": {
      "displayName": "Anjana B Reghu",
      "userId": "05439852886024039531"
     },
     "user_tz": -330
    },
    "id": "wT0uIhGcZaeh",
    "outputId": "897f49a7-7c04-460c-bfce-6751418479f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from pyngrok import ngrok\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "ngrok.set_auth_token(os.getenv(\"NGROK_AUTH_TOKEN\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52427,
     "status": "ok",
     "timestamp": 1743940644028,
     "user": {
      "displayName": "Anjana B Reghu",
      "userId": "05439852886024039531"
     },
     "user_tz": -330
    },
    "id": "KcYZ7VFiZci7",
    "outputId": "d7aff99c-bfd1-43cb-ffad-0ee233f80851"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "Flask server started on http://0.0.0.0:5000\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://172.28.0.12:5000\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Public URL: NgrokTunnel: \"https://35d6-34-125-38-149.ngrok-free.app\" -> \"http://localhost:5000\"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import time\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import os\n",
    "from unsloth import FastLanguageModel, get_chat_template, is_bfloat16_supported\n",
    "from unsloth.chat_templates import standardize_sharegpt, train_on_responses_only\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "import torch\n",
    "from transformers import TextIteratorStreamer\n",
    "from threading import Thread\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "# Add these imports at the top of your file\n",
    "import zipfile\n",
    "import io\n",
    "import shutil\n",
    "from flask import send_file\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "CORS(app, resources={r\"/*\": {\"origins\": \"*\"}})  # Enable CORS for all routes\n",
    "\n",
    "# Global variables\n",
    "model = None\n",
    "tokenizer = None\n",
    "alpaca_prompt = '''You are {name}, a student currently in semester {current_semester} of a {bachelor_specialization} bachelor's at {bachelor_university}, with a CGPA of {current_cgpa}. You have not completed your bachelor's yet. You live at {address}, and your hometown is {hometown}.\n",
    "\n",
    "  ### Education:\n",
    "  - High School: {high_school}, studied {_10th_subjects_count} subjects ({_10th_subjects}), scored {_10th_score}.\n",
    "  - Higher Secondary: {higher_secondary}, studied {_12th_subjects_count} subjects ({_12th_subjects}), scored {_12th_score}.\n",
    "\n",
    "  ### Skills:\n",
    "  - Programming: {skills}\n",
    "  - Tech Stacks: {tech_stacks}\n",
    "\n",
    "  ### Projects:\n",
    "  {proj}\n",
    "\n",
    "  ### Certifications:\n",
    "  {certi}\n",
    "\n",
    "  ### Instruction:\n",
    "  Answer the following question as {name} using only the information provided above. Do not speculate, invent details, or answer questions unrelated to this data. If the question cannot be answered with the given information, politely say, \"I donâ€™t have enough information to answer that.\"\n",
    "  ### Input:\n",
    "  {input}\n",
    "  ### Response:\n",
    "\n",
    "  '''\n",
    "user_details = None\n",
    "dataset_path = \"/content/drive/MyDrive/Resources/dataset.json\"\n",
    "model_save_path = \"/content/drive/MyDrive/Resources/lora_model\"\n",
    "\n",
    "# Create dataset from user info\n",
    "\n",
    "training_progress = {\"progress\": 0, \"status\": \"idle\"}\n",
    "# Generator functions for QA pairs\n",
    "def generate_name_qa(name):\n",
    "    questions = [\"Whatâ€™s your name?\", \"Can you tell me your name?\", \"Who are you?\"]\n",
    "    answers = [f\"My name is {name}.\", f\"Iâ€™m {name}.\", f\"Sure, my name is {name}.\"]\n",
    "    return [(q, a) for q in questions for a in answers]\n",
    "\n",
    "def generate_address_qa(address):\n",
    "    questions = [\n",
    "        \"Where do you live?\",\n",
    "        \"What is your address?\",\n",
    "        \"Where are you located?\",\n",
    "        \"Whatâ€™s your current address?\",\n",
    "        \"Where do you reside?\"\n",
    "    ]\n",
    "    return [(q, f\"I live at {address}.\") for q in questions]\n",
    "\n",
    "def generate_hometown_qa(hometown):\n",
    "    questions = [\"Where are you from?\", \"Whatâ€™s your hometown?\", \"Where did you grow up?\"]\n",
    "    return [(q, f\"Iâ€™m from {hometown}.\") for q in questions]\n",
    "\n",
    "def generate_education_qa(education):\n",
    "    qa_pairs = [\n",
    "        (f\"How many subjects were there in your 10th grade?\", f\"There were {education['10th_subjects_count']} subjects in my 10th grade.\"),\n",
    "        (f\"What subjects did you study in 10th grade?\", f\"In 10th grade, I studied {', '.join(education['10th_subjects'])}.\"),\n",
    "        (f\"What was your total score in 10th grade?\", f\"I scored {education['10th_score']} in 10th grade.\"),\n",
    "        (f\"How many subjects were there in your 12th grade?\", f\"There were {education['12th_subjects_count']} subjects in my 12th grade.\"),\n",
    "        (f\"What subjects did you study in 12th grade?\", f\"In 12th grade, I studied {', '.join(education['12th_subjects'])}.\"),\n",
    "        (f\"What was your total score in 12th grade?\", f\"I scored {education['12th_score']} in 12th grade.\"),\n",
    "        (f\"Where did you complete your 10th grade?\", f\"I completed my 10th grade at {education['high_school']}.\"),\n",
    "        (f\"Where did you complete your 12th grade?\", f\"I completed my 12th grade at {education['higher_secondary']}.\")\n",
    "    ]\n",
    "    if education[\"bachelors_status\"] == \"yes\":\n",
    "        qa_pairs.append((\"Whatâ€™s your Bachelorâ€™s degree?\", f\"I have a Bachelorâ€™s in {education['bachelor_specialization']} from {education['bachelor_university']} with a CGPA of {education['final_cgpa']}.\"))\n",
    "    else:\n",
    "        qa_pairs.extend([\n",
    "            (\"Which semester are you in right now?\", f\"Iâ€™m in my {education['current_semester']} semester.\"),\n",
    "            (\"Whatâ€™s your current CGPA?\", f\"My current CGPA is {education['current_cgpa']}.\")\n",
    "        ])\n",
    "    return qa_pairs\n",
    "\n",
    "def generate_skills_qa(skills):\n",
    "    questions = [\n",
    "        \"What are your skills?\",\n",
    "        \"What skills do you have?\",\n",
    "        \"What are you good at?\",\n",
    "        \"Tell me about your abilities.\"\n",
    "    ]\n",
    "    return [(q, f\"My skills include {', '.join(skills)}.\") for q in questions]\n",
    "\n",
    "def generate_certifications_qa(certifications):\n",
    "    questions = [\"What certifications do you have?\"]\n",
    "    cert_names = [c[\"name\"] for c in certifications]\n",
    "    qa_pairs = [(questions[0], f\"I have certifications such as {', '.join(cert_names)}.\")]\n",
    "    for c in certifications:\n",
    "        cert_questions = [\n",
    "            f\"Tell me about your {c['name']} certification.\",\n",
    "            f\"What is your {c['name']} certification?\",\n",
    "            f\"Can you explain your {c['name']} cert?\"\n",
    "        ]\n",
    "        qa_pairs.extend([(q, f\"I earned the {c['name']} certification from {c['issuer']} in {c['year']}.\") for q in cert_questions])\n",
    "    return qa_pairs\n",
    "\n",
    "def generate_projects_qa(projects):\n",
    "    questions = [\"What projects have you worked on?\"]\n",
    "    project_names = [p[\"name\"] for p in projects]\n",
    "    qa_pairs = [(questions[0], f\"I have worked on projects such as {', '.join(project_names)}.\")]\n",
    "    for p in projects:\n",
    "        proj_questions = [\n",
    "            f\"Tell me about {p['name']}.\",\n",
    "            f\"What is {p['name']}?\",\n",
    "            f\"Can you describe your {p['name']} project?\",\n",
    "            f\"What did you do in {p['name']}?\"\n",
    "        ]\n",
    "        qa_pairs.extend([(q, f\"{p['name']} is a project where {p['description']}.\") for q in proj_questions])\n",
    "    return qa_pairs\n",
    "\n",
    "def generate_tech_stacks_qa(tech_stacks):\n",
    "    questions = [\"What tech stacks are you comfortable with?\", \"Which technologies do you use?\"]\n",
    "    return [(q, f\"Iâ€™m comfortable with {', '.join(tech_stacks)}.\") for q in questions]\n",
    "\n",
    "def generate_profession_qa(profession):\n",
    "    questions = [\"What do you do?\", \"Whatâ€™s your profession?\"]\n",
    "    return [(q, f\"Iâ€™m a {profession['title']}, working on {profession['description']}.\") for q in questions]\n",
    "\n",
    "def create_user_details(details): # Store for later use\n",
    "\n",
    "    # Process education data\n",
    "    education = {\n",
    "    \"high_school\": details.get(\"Where did you complete your 10th grade? \", \"\"),\n",
    "    \"10th_subjects_count\": details.get(\"How many subjects in 10th grade? \", \"\"),\n",
    "    \"10th_subjects\": details.get(\"Subjects in 10th grade (comma-separated): \", \"\").split(\",\"),\n",
    "    \"10th_score\": details.get(\"Total score in 10th grade: \", \"\"),\n",
    "    \"higher_secondary\": details.get(\"Where did you complete your 12th grade? \", \"\"),\n",
    "    \"12th_subjects_count\": details.get(\"How many subjects in 12th grade? \", \"\"),\n",
    "    \"12th_subjects\": details.get(\"Subjects in 12th grade (comma-separated): \", \"\").split(\",\"),\n",
    "    \"12th_score\": details.get(\"Total score in 12th grade: \", \"\"),\n",
    "    \"bachelor_university\": details.get(\"Where are you pursuing your Bachelor's? \", \"\"),\n",
    "    \"bachelor_specialization\": details.get(\"What's your Bachelor's specialization? \", \"\"),\n",
    "    \"bachelors_status\": details.get(\"Completed your Bachelor's? (yes/no): \", \"no\").lower(),\n",
    "    \"current_semester\": details.get(\"What is your current semester? \", \"\"),\n",
    "    \"current_cgpa\": details.get(\"What is your current CGPA? \", \"\")\n",
    "    }\n",
    "\n",
    "    # Process project data\n",
    "    num_projects = int(details.get(\"How many projects do you want to add? \", \"0\"))\n",
    "    projects = []\n",
    "    for i in range(num_projects):\n",
    "        name_key = f\"Enter name of project {i + 1}: \"\n",
    "        desc_key = f\"Enter description for project {i + 1}: \"\n",
    "        if name_key in details and desc_key in details:\n",
    "            projects.append({\n",
    "                \"name\": details[name_key],\n",
    "                \"description\": details[desc_key]\n",
    "            })\n",
    "\n",
    "    # Process certification data\n",
    "    num_certs = int(details.get(\"How many certificates do you want to add? \", \"0\"))\n",
    "    certifications = []\n",
    "    for i in range(num_certs):\n",
    "        name_key = f\"Enter name of certificate {i + 1}: \"\n",
    "        issuer_key = f\"Enter issuing organization for certificate {i + 1}: \"\n",
    "        year_key = f\"Enter year of completion for certificate {i + 1}: \"\n",
    "        if name_key in details and issuer_key in details and year_key in details:\n",
    "            certifications.append({\n",
    "                \"name\": details[name_key],\n",
    "                \"issuer\": details[issuer_key],\n",
    "                \"year\": details[year_key]\n",
    "            })\n",
    "\n",
    "    # Create formatted data structure\n",
    "    formatted_details = {\n",
    "        \"name\": details.get(\"Enter your name: \", \"\"),\n",
    "        \"address\": details.get(\"Enter your address: \", \"\"),\n",
    "        \"hometown\": details.get(\"Enter your hometown: \", \"\"),\n",
    "        \"education\": education,\n",
    "        \"tech_stacks\": details.get(\"Tech stacks you're comfortable with (comma-separated): \", \"\").split(\",\"),\n",
    "        \"skills\": details.get(\"Enter your Skills (comma-separated): \", \"\").split(\",\"),\n",
    "        \"projects\": projects,\n",
    "        \"certifications\": certifications\n",
    "    }\n",
    "\n",
    "    # Store the formatted details for chat function\n",
    "    with open(\"/content/drive/MyDrive/Resources/user_details.json\", \"w\") as f:\n",
    "        json.dump(formatted_details, f, indent=4)\n",
    "\n",
    "    return formatted_details\n",
    "\n",
    "\n",
    "def create_dataset(details,desired_samples=1000):\n",
    "    qa_pairs = []\n",
    "    qa_pairs.extend(generate_name_qa(details[\"name\"]))\n",
    "    qa_pairs.extend(generate_address_qa(details[\"address\"]))\n",
    "    qa_pairs.extend(generate_hometown_qa(details[\"hometown\"]))\n",
    "    qa_pairs.extend(generate_education_qa(details[\"education\"]))\n",
    "    qa_pairs.extend(generate_tech_stacks_qa(details[\"tech_stacks\"]))\n",
    "    qa_pairs.extend(generate_certifications_qa(details[\"certifications\"]))\n",
    "    qa_pairs.extend(generate_skills_qa(details[\"skills\"]))\n",
    "    qa_pairs.extend(generate_projects_qa(details[\"projects\"]))\n",
    "\n",
    "    if \"profession\" in details:\n",
    "        qa_pairs.extend(generate_profession_qa(details[\"profession\"]))\n",
    "\n",
    "    # Remove duplicates and limit to desired samples\n",
    "    unique_qa_pairs = list(set(qa_pairs))\n",
    "    dataset = []\n",
    "    for q, a in unique_qa_pairs[:desired_samples]:\n",
    "        dataset.append({\"role\": \"user\", \"content\": q})\n",
    "        dataset.append({\"role\": \"assistant\", \"content\": a})\n",
    "\n",
    "    # Save to JSON\n",
    "    with open(\"/content/drive/MyDrive/Resources/dataset.json\", \"w\") as f:\n",
    "        json.dump(dataset, f, indent=4)\n",
    "    print(f\"Generated {len(dataset)//2} unique QA pairs. Saved to 'dataset.json'.\")\n",
    "\n",
    "    # Show a few examples\n",
    "    for i in range(0, min(6, len(dataset)), 2):\n",
    "        print(f\"User: {dataset[i]['content']}\")\n",
    "        print(f\"Assistant: {dataset[i+1]['content']}\")\n",
    "        print(\"---\")\n",
    "\n",
    "    return \"success\"\n",
    "\n",
    "class ProgressCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        global training_progress\n",
    "        if state.max_steps > 0:\n",
    "            progress = (state.global_step / state.max_steps) * 100\n",
    "            training_progress[\"progress\"] = int(progress)\n",
    "            training_progress[\"status\"] = \"training\"\n",
    "            print(f\"Training progress: {int(progress)}%\")\n",
    "\n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        global training_progress\n",
    "        training_progress[\"progress\"] = 100\n",
    "        training_progress[\"status\"] = \"completed\"\n",
    "        print(\"Training completed\")\n",
    "\n",
    "def train_model():\n",
    "    global training_progress, model, tokenizer\n",
    "    model,tokenizer = None,None\n",
    "    try:\n",
    "        # Load the pre-trained model and tokenizer with 4-bit quantization\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=\"unsloth/Llama-3.2-3B-Instruct\",\n",
    "            max_seq_length=2048,\n",
    "            dtype=None,\n",
    "            load_in_4bit=True,\n",
    "        )\n",
    "\n",
    "        # Configure LoRA for efficient fine-tuning\n",
    "        model = FastLanguageModel.get_peft_model(\n",
    "            model,\n",
    "            r=16,\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0,\n",
    "            bias=\"none\",\n",
    "            use_gradient_checkpointing=\"unsloth\",\n",
    "            random_state=3407,\n",
    "            use_rslora=False,\n",
    "            loftq_config=None,\n",
    "        )\n",
    "\n",
    "        # Apply the appropriate chat template to the tokenizer\n",
    "        tokenizer = get_chat_template(\n",
    "            tokenizer,\n",
    "            chat_template=\"llama-3.1\",\n",
    "        )\n",
    "\n",
    "        # Define the function to format prompts with the chat template\n",
    "        def formatting_prompts_func(examples):\n",
    "            convos = examples[\"conversations\"]\n",
    "            texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n",
    "            return {\"text\": texts}\n",
    "\n",
    "        # Load and prepare the dataset\n",
    "        dataset_path = \"/content/drive/MyDrive/Resources/dataset.json\"  # Adjust path as needed\n",
    "        dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
    "\n",
    "        def restructure_data(example):\n",
    "            return {\n",
    "                \"conversations\": [\n",
    "                    {\"role\": example[\"role\"], \"content\": example[\"content\"]}\n",
    "                ]\n",
    "            }\n",
    "\n",
    "        dataset = dataset.map(restructure_data, batched=False)\n",
    "        dataset = standardize_sharegpt(dataset)\n",
    "        dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "        dataset = dataset.remove_columns([col for col in dataset.column_names if col != \"text\"])\n",
    "\n",
    "        # Set up the supervised fine-tuning trainer\n",
    "        trainer = SFTTrainer(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            train_dataset=dataset,\n",
    "            dataset_text_field=\"text\",\n",
    "            max_seq_length=2048,\n",
    "            data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n",
    "            dataset_num_proc=2,\n",
    "            packing=False,\n",
    "            args=TrainingArguments(\n",
    "                per_device_train_batch_size=2,\n",
    "                gradient_accumulation_steps=4,\n",
    "                warmup_steps=5,\n",
    "                max_steps=60,  # Number of training steps\n",
    "                learning_rate=2e-4,\n",
    "                fp16=not is_bfloat16_supported(),\n",
    "                bf16=is_bfloat16_supported(),\n",
    "                logging_steps=1,\n",
    "                optim=\"adamw_8bit\",\n",
    "                weight_decay=0.01,\n",
    "                lr_scheduler_type=\"linear\",\n",
    "                seed=3407,\n",
    "                output_dir=\"outputs\",\n",
    "                report_to=\"none\",\n",
    "            ),\n",
    "            callbacks=[ProgressCallback()],\n",
    "        )\n",
    "\n",
    "        # Modify trainer to train only on assistant responses\n",
    "        trainer = train_on_responses_only(\n",
    "            trainer,\n",
    "            instruction_part=\"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "            response_part=\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "        )\n",
    "\n",
    "        # Start the training process\n",
    "        trainer_stats = trainer.train()\n",
    "\n",
    "        # Update progress upon completion\n",
    "        training_progress[\"progress\"] = 100\n",
    "        training_progress[\"status\"] = \"completed\"\n",
    "        print(\"Training completed, status updated to 'completed'\")\n",
    "\n",
    "        # Save the fine-tuned model and tokenizer\n",
    "        model_save_path = \"/content/drive/MyDrive/Resources/lora_model\"\n",
    "        model.save_pretrained(model_save_path)\n",
    "        tokenizer.save_pretrained(model_save_path)\n",
    "        print(\"Model saved successfully\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle any errors during training\n",
    "        training_progress[\"status\"] = \"error\"\n",
    "        training_progress[\"message\"] = str(e)\n",
    "        training_progress[\"progress\"] = 0\n",
    "        print(f\"Training error: {str(e)}\")\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"Load the model at startup and keep it in memory.\"\"\"\n",
    "    global model, tokenizer, alpaca_prompt, model_save_path\n",
    "    if model is not None and tokenizer is not None:\n",
    "        print(\"Model already loaded at startup.\")\n",
    "        return\n",
    "\n",
    "    print(\"Loading model at startup...\")\n",
    "    if not os.path.exists(model_save_path):\n",
    "        raise FileNotFoundError(f\"Model directory {model_save_path} does not exist. Ensure training completed.\")\n",
    "\n",
    "    try:\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=model_save_path,\n",
    "            max_seq_length=2048,\n",
    "            dtype=None,\n",
    "            load_in_4bit=True,\n",
    "        )\n",
    "        FastLanguageModel.for_inference(model)\n",
    "        alpaca_prompt = '''You are {name}, a student currently in semester {current_semester} of a {bachelor_specialization} bachelor's at {bachelor_university}, with a CGPA of {current_cgpa}. You have not completed your bachelor's yet. You live at {address}, and your hometown is {hometown}.\n",
    "\n",
    "  ### Education:\n",
    "  - High School: {high_school}, studied {_10th_subjects_count} subjects ({_10th_subjects}), scored {_10th_score}.\n",
    "  - Higher Secondary: {higher_secondary}, studied {_12th_subjects_count} subjects ({_12th_subjects}), scored {_12th_score}.\n",
    "\n",
    "  ### Skills:\n",
    "  - Programming: {skills}\n",
    "  - Tech Stacks: {tech_stacks}\n",
    "\n",
    "  ### Projects:\n",
    "  {proj}\n",
    "\n",
    "  ### Certifications:\n",
    "  {certi}\n",
    "\n",
    "  ### Instruction:\n",
    "  Answer the following question as {name} using only the information provided above. Do not speculate, invent details, or answer questions unrelated to this data. If the question cannot be answered with the given information, politely say, \"I donâ€™t have enough information to answer that.\"\n",
    "  ### Input:\n",
    "  {input}\n",
    "  ### Response:\n",
    "\n",
    "  '''\n",
    "        print(\"Model loaded successfully at startup!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load model: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def generate_response(query):\n",
    "    global model, tokenizer, alpaca_prompt, user_details\n",
    "    if model is None or tokenizer is None:\n",
    "        load_model()\n",
    "\n",
    "    # Load user details if not already loaded\n",
    "    if user_details is None:\n",
    "        try:\n",
    "            with open(\"/content/drive/MyDrive/Resources/user_details.json\", \"r\") as f:\n",
    "                user_details = json.load(f)\n",
    "        except Exception as e:\n",
    "            return f\"Error: User details not found. Please complete the fine-tuning process first. Details: {str(e)}\"\n",
    "\n",
    "    # Extract and format personal details\n",
    "    name = user_details['name'].strip()\n",
    "    education = user_details['education']\n",
    "    tech_stacks = \", \".join(user_details['tech_stacks']).replace(\"AWS,AZURE\", \"AWS, Azure\")\n",
    "    projects = user_details['projects']\n",
    "    certifications = user_details['certifications']\n",
    "    skills = \", \".join(user_details['skills'])\n",
    "    hometown = user_details['hometown'].strip()\n",
    "    address = user_details['address'].strip()\n",
    "\n",
    "    # Format education details with proper capitalization and structure\n",
    "    high_school = education['high_school'].strip()\n",
    "    higher_secondary = education['higher_secondary'].strip()\n",
    "    bachelor_university = education['bachelor_university'].strip()\n",
    "    current_semester = education.get('current_semester', '3rd').strip()\n",
    "    current_cgpa = education.get('current_cgpa', '8.5').strip()\n",
    "    _10th_subjects_count = education['10th_subjects_count'].strip()\n",
    "    _10th_subjects = \", \".join(education['10th_subjects']).strip()\n",
    "    _10th_score = education['10th_score'].strip()\n",
    "    _12th_subjects_count = education['12th_subjects_count'].strip()\n",
    "    _12th_subjects = \", \".join(education['12th_subjects']).strip()\n",
    "    _12th_score = education['12th_score'].strip()\n",
    "    bachelor_specialization = education['bachelor_specialization'].strip()\n",
    "\n",
    "    # Enhanced projects formatting with more professional descriptions\n",
    "    proj = \"\"\n",
    "    for i, p in enumerate(projects, 1):\n",
    "        project_name = p['name'].strip()\n",
    "        project_desc = p['description'].strip()\n",
    "        # Ensure description ends with period\n",
    "        if not project_desc.endswith('.'):\n",
    "            project_desc += '.'\n",
    "        proj += f\"- Project {i}: {project_name} - {project_desc}\\n\"\n",
    "\n",
    "    # Enhanced certifications formatting with consistent styling\n",
    "    certi = \"\"\n",
    "    for i, c in enumerate(certifications, 1):\n",
    "        cert_name = c['name'].strip()\n",
    "        cert_issuer = c['issuer'].strip()\n",
    "        cert_year = c['year'].strip()\n",
    "        certi += f\"- Certification {i}: {cert_name} from {cert_issuer} ({cert_year})\\n\"\n",
    "\n",
    "    # Populate the prompt with all formatted information\n",
    "    prompt = alpaca_prompt.format(\n",
    "        name=name,\n",
    "        current_semester=current_semester,\n",
    "        bachelor_university=bachelor_university,\n",
    "        bachelor_specialization=bachelor_specialization,\n",
    "        current_cgpa=current_cgpa,\n",
    "        address=address,\n",
    "        hometown=hometown,\n",
    "        high_school=high_school,\n",
    "        _10th_subjects_count=_10th_subjects_count,\n",
    "        _10th_subjects=_10th_subjects,\n",
    "        _10th_score=_10th_score,\n",
    "        higher_secondary=higher_secondary,\n",
    "        _12th_subjects_count=_12th_subjects_count,\n",
    "        _12th_subjects=_12th_subjects,\n",
    "        _12th_score=_12th_score,\n",
    "        skills=skills,\n",
    "        tech_stacks=tech_stacks,\n",
    "        proj=proj,\n",
    "        certi=certi,\n",
    "        input=query\n",
    "    )\n",
    "    print(prompt)\n",
    "    # Enhanced generation parameters for more professional responses\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    # Increase max_new_tokens for more comprehensive responses\n",
    "    # Lower temperature for more focused and professional tone\n",
    "    generation_kwargs = dict(\n",
    "        inputs,\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=200,  # Increased for more detailed responses\n",
    "        do_sample=True,      # Enable sampling for natural language\n",
    "        top_p=0.92,          # Slightly more focused sampling\n",
    "        top_k=40,            # More restrictive for professional tone\n",
    "        temperature=0.7,     # Balanced between creativity and professionalism\n",
    "        repetition_penalty=1.1  # Avoid repetitive phrases\n",
    "    )\n",
    "\n",
    "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "\n",
    "    # Collect generated text\n",
    "    generated_text = \"\"\n",
    "    for new_text in streamer:\n",
    "        generated_text += new_text\n",
    "\n",
    "    # Post-process to ensure professional formatting\n",
    "    generated_text = generated_text.strip()\n",
    "\n",
    "    # Ensure response doesn't end mid-sentence\n",
    "    if generated_text and generated_text[-1] not in ['.', '!', '?', ':']:\n",
    "        # Find the last complete sentence\n",
    "        last_period = max(generated_text.rfind('.'), generated_text.rfind('!'), generated_text.rfind('?'))\n",
    "        if last_period > len(generated_text) * 0.7:  # Only trim if we're not losing too much content\n",
    "            generated_text = generated_text[:last_period+1]\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "# API Routes\n",
    "\n",
    "\n",
    "@app.route('/ping', methods=['GET'])\n",
    "def ping():\n",
    "    \"\"\"Simple endpoint to verify connectivity\"\"\"\n",
    "    return jsonify({\n",
    "        \"status\": \"connected\",\n",
    "        \"message\": \"Backend is running\",\n",
    "        \"training_status\": training_progress\n",
    "    })\n",
    "\n",
    "@app.route('/generate_dataset', methods=['POST'])\n",
    "def api_generate_dataset():\n",
    "    data = request.get_json()\n",
    "    if not data:\n",
    "        print(\"No data received in request\")\n",
    "        return jsonify({\"error\": \"No data provided\"}), 400\n",
    "\n",
    "    # Log the received data\n",
    "    print(\"Received data:\", json.dumps(data, indent=4))\n",
    "\n",
    "    try:\n",
    "        personal_data = create_user_details(data)\n",
    "        success = create_dataset(personal_data)\n",
    "        print(\"Dataset creation completed successfully\")\n",
    "        return jsonify({\n",
    "            \"success\": success,\n",
    "            \"message\": \"Dataset Creation Successful\",\n",
    "            \"status\": \"completed\"\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error during dataset creation: {str(e)}\")\n",
    "        return jsonify({\"error\": str(e), \"status\": \"error\"}), 500\n",
    "\n",
    "@app.route('/train_model', methods=['POST'])\n",
    "def api_train_model():\n",
    "    global training_progress\n",
    "    training_progress = {\"progress\": 0, \"status\": \"training\"}\n",
    "    try:\n",
    "        # Run training in a separate thread to avoid blocking\n",
    "        def run_training():\n",
    "            try:\n",
    "                train_model()\n",
    "            except Exception as e:\n",
    "                # Make sure exceptions in the thread are captured\n",
    "                training_progress[\"status\"] = \"error\"\n",
    "                training_progress[\"message\"] = str(e)\n",
    "                print(f\"Training error in thread: {str(e)}\")\n",
    "\n",
    "        train_thread = Thread(target=run_training)\n",
    "        train_thread.daemon = True  # Make thread daemon so it doesn't block app shutdown\n",
    "        train_thread.start()\n",
    "\n",
    "        return jsonify({\"success\": True, \"message\": \"Model training started\"})\n",
    "    except Exception as e:\n",
    "        training_progress[\"status\"] = \"error\"\n",
    "        training_progress[\"message\"] = str(e)\n",
    "        return jsonify({\"error\": str(e), \"success\": False}), 500\n",
    "\n",
    "@app.route('/load_model', methods=['POST'])\n",
    "def api_load_model():\n",
    "    try:\n",
    "        success = load_model()\n",
    "        return jsonify({\"success\": success, \"message\": \"Model loaded successfully\"})\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "@app.route('/chat', methods=['POST'])\n",
    "def api_chat():\n",
    "    data = request.get_json()\n",
    "    if not data or 'message' not in data:\n",
    "        return jsonify({\"error\": \"No message provided\"}), 400\n",
    "\n",
    "    try:\n",
    "        response = generate_response(data['message'])\n",
    "        return jsonify({\"response\": response})\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@app.route('/download_dataset', methods=['GET'])\n",
    "def download_dataset():\n",
    "    \"\"\"Endpoint to download the generated dataset\"\"\"\n",
    "    try:\n",
    "        # Check if the dataset file exists\n",
    "        dataset_file_path = \"/content/drive/MyDrive/Resources/dataset.json\"\n",
    "        if not os.path.exists(dataset_file_path):\n",
    "            return jsonify({\"error\": \"Dataset file not found\"}), 404\n",
    "\n",
    "        # Return the file as an attachment\n",
    "        return send_file(\n",
    "            dataset_file_path,\n",
    "            mimetype='application/json',\n",
    "            as_attachment=True,\n",
    "            download_name='dataset.json'\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": f\"Failed to download dataset: {str(e)}\"}), 500\n",
    "\n",
    "@app.route('/download_model', methods=['GET'])\n",
    "def download_model():\n",
    "    \"\"\"Endpoint to download the trained model as a zip file\"\"\"\n",
    "    try:\n",
    "        # Check if the model directory exists\n",
    "        model_dir_path = \"/content/drive/MyDrive/Resources/lora_model\"\n",
    "        if not os.path.exists(model_dir_path):\n",
    "            return jsonify({\"error\": \"Model directory not found\"}), 404\n",
    "\n",
    "        # Create a memory file for the zip\n",
    "        memory_file = io.BytesIO()\n",
    "\n",
    "        # Create the zip file\n",
    "        with zipfile.ZipFile(memory_file, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "            # Walk through the model directory and add all files\n",
    "            for root, dirs, files in os.walk(model_dir_path):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    arcname = os.path.relpath(file_path, os.path.dirname(model_dir_path))\n",
    "                    zipf.write(file_path, arcname)\n",
    "\n",
    "        # Reset file pointer\n",
    "        memory_file.seek(0)\n",
    "\n",
    "        # Return the zip file as an attachment\n",
    "        return send_file(\n",
    "            memory_file,\n",
    "            mimetype='application/zip',\n",
    "            as_attachment=True,\n",
    "            download_name='lora_model.zip'\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": f\"Failed to download model: {str(e)}\"}), 500\n",
    "\n",
    "\n",
    "\n",
    "@app.route('/progress', methods=['GET'])\n",
    "def api_progress():\n",
    "    global training_progress\n",
    "    # Add timestamp to help debug\n",
    "    response = training_progress.copy()\n",
    "    response[\"timestamp\"] = time.time()\n",
    "    return jsonify(response)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Start Flask in a separate thread\n",
    "    def run_flask():\n",
    "        app.run(host='0.0.0.0', port=5000, debug=False, use_reloader=False)\n",
    "\n",
    "    flask_thread = Thread(target=run_flask)\n",
    "    flask_thread.start()\n",
    "    print(\"Flask server started on http://0.0.0.0:5000\")\n",
    "\n",
    "    # Set up ngrok tunnel\n",
    "    public_url = ngrok.connect(5000, \"http\")\n",
    "    print(f\"Public URL: {public_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KpMxJjX8o2Qx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
